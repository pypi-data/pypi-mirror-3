

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pgmlearner &mdash; libpgm 1.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="libpgm 1.1 documentation" href="index.html" />
    <link rel="next" title="CPDtypes" href="CPDtypes.html" />
    <link rel="prev" title="sampleaggregator" href="sampleaggregator.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="CPDtypes.html" title="CPDtypes"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="sampleaggregator.html" title="sampleaggregator"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">libpgm 1.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-libpgm.pgmlearner">
<span id="pgmlearner"></span><h1>pgmlearner<a class="headerlink" href="#module-libpgm.pgmlearner" title="Permalink to this headline">¶</a></h1>
<p>This module provides tools to generate Bayesian networks that are &#8220;learned&#8221; from a data set. The learning process involves finding the Bayesian network that most accurately models data given as input &#8211; in other words, finding the Bayesian network that makes the data set most likely. There are two major parts of Bayesian network learning: structure learning and parameter learning. Structure learning means finding the graph that most accurately depicts the dependencies detected in the data. Parameter learning means adjusting the parameters of the CPDs in a graph skeleton to most accurately model the data. This module has tools for both of these tasks.</p>
<dl class="class">
<dt id="libpgm.pgmlearner.PGMLearner">
<em class="property">class </em><tt class="descclassname">libpgm.pgmlearner.</tt><tt class="descname">PGMLearner</tt><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is a machine with tools for learning Bayesian networks from data. It contains the <em>discrete_mle_estimateparams</em>, <em>lg_mle_estimateparams</em>, <em>discrete_constraint_estimatestruct</em>, <em>lg_constraint_estimatestruct</em>, <em>discrete_condind</em>, <em>discrete_estimatebn</em>, and <em>lg_estimatebn</em> methods.</p>
<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.discrete_mle_estimateparams">
<tt class="descname">discrete_mle_estimateparams</tt><big>(</big><em>graphskeleton</em>, <em>data</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.discrete_mle_estimateparams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.discrete_mle_estimateparams" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate parameters for a discrete Bayesian network with a structure given by <em>graphskeleton</em> in order to maximize the probability of data given by <em>data</em>. This function takes the following arguments:</p>
<blockquote>
<div><ol class="arabic">
<li><p class="first"><em>graphskeleton</em> &#8211; An instance of the <a class="reference internal" href="graphskeleton.html"><em>GraphSkeleton</em></a> class containing vertex and edge data.</p>
</li>
<li><p class="first"><em>data</em> &#8211; A list of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 'B',
        'SAT': 'lowscore',
        ...
    },
    ...
]</pre>
</div>
</li>
</ol>
</div></blockquote>
<p>This function normalizes the distribution of a node&#8217;s outcomes for each combination of its parents&#8217; outcomes. In doing so it creates an estimated tabular conditional probability distribution for each node. It then instantiates a <a class="reference internal" href="discretebayesiannetwork.html"><em>DiscreteBayesianNetwork</em></a> instance based on the <em>graphskeleton</em>, and modifies that instance&#8217;s <em>Vdata</em> attribute to reflect the estimated CPDs. It then returns the instance.</p>
<p>The Vdata attribute instantiated is in the format seen in <a class="reference internal" href="unittestdict.html"><em>discrete bayesian network</em></a>, as described in <a class="reference internal" href="discretebayesiannetwork.html"><em>discretebayesiannetwork</em></a>.</p>
<p>Usage example: this would learn parameters from a set of 200 discrete samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">libpgm.nodedata</span> <span class="kn">import</span> <span class="n">NodeData</span>
<span class="kn">from</span> <span class="nn">libpgm.graphskeleton</span> <span class="kn">import</span> <span class="n">GraphSkeleton</span>
<span class="kn">from</span> <span class="nn">libpgm.discretebayesiannetwork</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.pgmlearner</span> <span class="kn">import</span> <span class="n">PGMLearner</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># estimate parameters from data and skeleton</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">discrete_mle_estimateparams</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">Vdata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.lg_mle_estimateparams">
<tt class="descname">lg_mle_estimateparams</tt><big>(</big><em>graphskeleton</em>, <em>data</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.lg_mle_estimateparams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.lg_mle_estimateparams" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate parameters for a linear Gaussian Bayesian network with a structure given by <em>graphskeleton</em> in order to maximize the probability of data given by <em>data</em>. This function takes the following arguments:</p>
<blockquote>
<div><ol class="arabic">
<li><p class="first"><em>graphskeleton</em> &#8211; An instance of the <a class="reference internal" href="graphskeleton.html"><em>GraphSkeleton</em></a> class containing vertex and edge data.</p>
</li>
<li><p class="first"><em>data</em> &#8211; A list of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 74.343,
        'Intelligence': 29.545,
        ...
    },
    ...
]</pre>
</div>
</li>
</ol>
</div></blockquote>
<p>The algorithm used to calculate the linear Gaussian parameters is beyond the scope of this documentation &#8211; for a full explanation, cf. Koller et al. 729. After the parameters are calculated, the program instantiates a <a class="reference internal" href="discretebayesiannetwork.html"><em>DiscreteBayesianNetwork</em></a> instance based on the <em>graphskeleton</em>, and modifies that instance&#8217;s <em>Vdata</em> attribute to reflect the estimated CPDs. It then returns the instance.</p>
<p>The Vdata attribute instantiated is in the format seen in the input file example <a class="reference internal" href="unittestdict.html"><em>discrete bayesian network</em></a>, as described in <a class="reference internal" href="discretebayesiannetwork.html"><em>discretebayesiannetwork</em></a>.</p>
<p>Usage example: this would learn parameters from a set of 200 linear Gaussian samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">libpgm.nodedata</span> <span class="kn">import</span> <span class="n">NodeData</span>
<span class="kn">from</span> <span class="nn">libpgm.graphskeleton</span> <span class="kn">import</span> <span class="n">GraphSkeleton</span>
<span class="kn">from</span> <span class="nn">libpgm.lgbayesiannetwork</span> <span class="kn">import</span> <span class="n">LGBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.pgmlearner</span> <span class="kn">import</span> <span class="n">PGMLearner</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestlgdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">lgbn</span> <span class="o">=</span> <span class="n">LGBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">lgbn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># estimate parameters</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">lg_mle_estimateparams</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">Vdata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.discrete_constraint_estimatestruct">
<tt class="descname">discrete_constraint_estimatestruct</tt><big>(</big><em>data</em>, <em>pvalparam=0.050000000000000003</em>, <em>indegree=1</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.discrete_constraint_estimatestruct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.discrete_constraint_estimatestruct" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn a Bayesian network structure from discrete data given by <em>data</em>, using constraint-based approaches. This function first calculates all the independencies and conditional independencies present between variables in the data. To calculate dependencies, it uses the <em>discrete_condind</em> method on each pair of variables, conditioned on other sets of variables of size <em>indegree</em> or smaller, to generate a chi-squared result and a p-value. If this p-value is less than <em>pvalparam</em>, the pair of variables are considered dependent conditioned on the variable set. Once all true dependencies &#8211; pairs of variables that are dependent no matter what they are conditioned by &#8211; are found, the algorithm uses these dependencies to construct a directed acyclic graph. It returns this DAG in the form of a <a class="reference internal" href="graphskeleton.html"><em>GraphSkeleton</em></a> class.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ol class="first last arabic">
<li><p class="first"><em>data</em> &#8211; An array of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 'B',
        'SAT': 'lowscore',
        ...
    },
    ...
]</pre>
</div>
</li>
<li><p class="first"><em>pvalparam</em> &#8211; (Optional, default is 0.05) The p-value below which to consider something significantly unlikely. A common number used is 0.05. This is passed to <em>discrete_condind</em> when it is called.</p>
</li>
<li><p class="first"><em>indegree</em> &#8211; (Optional, default is 1) The upper bound on the size of a witness set (see Koller et al. 85). If this is larger than 1, a huge amount of samples in <em>data</em> are required to avoid a divide-by-zero error.</p>
</li>
</ol>
</dd>
</dl>
<p>Usage example: this would learn structure from a set of 8000 discrete samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">libpgm.nodedata</span> <span class="kn">import</span> <span class="n">NodeData</span>
<span class="kn">from</span> <span class="nn">libpgm.graphskeleton</span> <span class="kn">import</span> <span class="n">GraphSkeleton</span>
<span class="kn">from</span> <span class="nn">libpgm.discretebayesiannetwork</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.pgmlearner</span> <span class="kn">import</span> <span class="n">PGMLearner</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">8000</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># estimate structure</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">discrete_constraint_estimatestruct</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.lg_constraint_estimatestruct">
<tt class="descname">lg_constraint_estimatestruct</tt><big>(</big><em>data</em>, <em>pvalparam=0.050000000000000003</em>, <em>bins=10</em>, <em>indegree=1</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.lg_constraint_estimatestruct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.lg_constraint_estimatestruct" title="Permalink to this definition">¶</a></dt>
<dd><p>Learn a Bayesian network structure from linear Gaussian data given by <em>data</em> using constraint-based approaches. This function works by discretizing the linear Gaussian data into <em>bins</em> number of bins, and running the <em>discrete_constraint_estimatestruct</em> method on that discrete data with <em>pvalparam</em> and <em>indegree</em> as arguments. It returns the <a class="reference internal" href="graphskeleton.html"><em>GraphSkeleton</em></a> instance returned by this function.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ol class="first last arabic">
<li><p class="first"><em>data</em> &#8211; An array of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 78.3223,
        'SAT': 56.33,
        ...
    },
    ...
]</pre>
</div>
</li>
<li><p class="first"><em>pvalparam</em> &#8211; (Optional, default is 0.05) The p-value below which to consider something significantly unlikely. A common number used is 0.05</p>
</li>
<li><p class="first"><em>bins</em> &#8211; (Optional, default is 10) The number of bins to discretize the data into. The method is to find the highest and lowest value, divide that interval uniformly into a certain number of bins, and place the data inside. This number must be chosen carefully in light of the number of trials. There must be at least 5 trials in every bin, with more if the indegree is increased.</p>
</li>
<li><p class="first"><em>indegree</em> &#8211; (Optional, default is 1) The upper bound on the size of a witness set (see Koller et al. 85). If this is larger than 1, a huge amount of trials are required to avoid a divide-by-zero error.</p>
</li>
</ol>
</dd>
</dl>
<p>The number of bins and indegree must be chosen carefully based on the size and nature of the data set. Too many bins will lead to not enough data per bin, while too few bins will lead to dependencies not getting noticed.</p>
<p>Usage example: this would learn structure from a set of 8000 linear Gaussian samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">libpgm.nodedata</span> <span class="kn">import</span> <span class="n">NodeData</span>
<span class="kn">from</span> <span class="nn">libpgm.graphskeleton</span> <span class="kn">import</span> <span class="n">GraphSkeleton</span>
<span class="kn">from</span> <span class="nn">libpgm.lgbayesiannetwork</span> <span class="kn">import</span> <span class="n">LGBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.pgmlearner</span> <span class="kn">import</span> <span class="n">PGMLearner</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">lgbn</span> <span class="o">=</span> <span class="n">LGBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">lgbn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">8000</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># estimate structure</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">lg_constraint_estimatestruct</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.discrete_condind">
<tt class="descname">discrete_condind</tt><big>(</big><em>data</em>, <em>X</em>, <em>Y</em>, <em>U</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.discrete_condind"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.discrete_condind" title="Permalink to this definition">¶</a></dt>
<dd><p>Test how independent a variable <em>X</em> and a variable <em>Y</em> are in a discrete data set given by <em>data</em>, where the independence is conditioned on a set of variables given by <em>U</em>. This method works by assuming as a null hypothesis that the variables are conditionally independent on <em>U</em>, and thus that:</p>
<div class="math">
\[P(X, Y, U) = P(U) \cdot P(X|U) \cdot P(Y|U) \]</div>
<p>It tests the deviance of the data from this null hypothesis, returning the result of a chi-square test and a p-value.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ol class="first last arabic">
<li><p class="first"><em>data</em> &#8211; An array of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 'B',
        'SAT': 'lowscore',
        ...
    },
    ...
]</pre>
</div>
</li>
<li><p class="first"><em>X</em> &#8211; A variable whose dependence on Y we are testing given U.</p>
</li>
<li><p class="first"><em>Y</em> &#8211; A variable whose dependence on X we are testing given U.</p>
</li>
<li><p class="first"><em>U</em> &#8211; A list of variables that are given.</p>
</li>
</ol>
</dd>
<dt>Returns:</dt>
<dd><ol class="first last arabic">
<li><dl class="first docutils">
<dt><em>chi</em> &#8211; The result of the chi-squared test on the data. This is a</dt>
<dd><p class="first last">measure of the deviance of the actual distribution of X and
Y given U from the expected distribution of X and Y given U.
Since the null hypothesis is that X and Y are independent 
given U, the expected distribution is that <span class="math">\(P(X, Y, U) =
P(U) P(X | U) P (Y | U)\)</span>.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><em>pval</em> &#8211; The p-value of the test, meaning the probability of</dt>
<dd><p class="first last">attaining a chi-square result as extreme as or more extreme
than the one found, assuming that the null hypothesis is
true. (e.g., a p-value of .05 means that if X and Y were 
independent given U, the chance of getting a chi-squared
result this high or higher are .05)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><em>U</em> &#8211; The &#8216;witness&#8217; of X and Y&#8217;s independence. This is the variable</dt>
<dd><p class="first last">that, when it is known, leaves X and Y independent.</p>
</dd>
</dl>
</li>
</ol>
</dd>
</dl>
<p>For more information see Koller et al. 790.</p>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.discrete_estimatebn">
<tt class="descname">discrete_estimatebn</tt><big>(</big><em>data</em>, <em>pvalparam=0.050000000000000003</em>, <em>indegree=1</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.discrete_estimatebn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.discrete_estimatebn" title="Permalink to this definition">¶</a></dt>
<dd><p>Fully learn a Bayesian network from discrete data given by <em>data</em>. This function combines the <em>discrete_constraint_estimatestruct</em> method (where it passes in the <em>pvalparam</em> and <em>indegree</em> arguments) with the <em>discrete_mle_estimateparams</em> method. It returns a complete <a class="reference internal" href="discretebayesiannetwork.html"><em>DiscreteBayesianNetwork</em></a> class instance learned from the data.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ol class="first last arabic">
<li><p class="first"><em>data</em> &#8211; An array of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 'B',
        'SAT': 'lowscore',
        ...
    },
    ...
]</pre>
</div>
</li>
<li><p class="first"><em>pvalparam</em> &#8211; The p-value below which to consider something significantly unlikely. A common number used is 0.05</p>
</li>
<li><p class="first"><em>indegree</em> &#8211; The upper bound on the size of a witness set (see Koller et al. 85). If this is larger than 1, a huge amount of trials are required to avoid a divide-by- zero error.</p>
</li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="libpgm.pgmlearner.PGMLearner.lg_estimatebn">
<tt class="descname">lg_estimatebn</tt><big>(</big><em>data</em>, <em>pvalparam=0.050000000000000003</em>, <em>bins=10</em>, <em>indegree=1</em><big>)</big><a class="reference internal" href="_modules/libpgm/pgmlearner.html#PGMLearner.lg_estimatebn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#libpgm.pgmlearner.PGMLearner.lg_estimatebn" title="Permalink to this definition">¶</a></dt>
<dd><p>Fully learn a Bayesian network from linear Gaussian data given by <em>data</em>. This function combines the <em>lg_constraint_estimatestruct</em> method (where it passes in the <em>pvalparam</em>, <em>bins</em>, and <em>indegree</em> arguments) with the <em>lg_mle_estimateparams</em> method. It returns a complete <a class="reference internal" href="discretebayesiannetwork.html"><em>LGBayesianNetwork</em></a> class instance learned from the data.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ol class="first last arabic">
<li><p class="first"><em>data</em> &#8211; An array of dicts containing samples from the network in {vertex: value} format. Example:</p>
<div class="highlight-python"><pre>[
    {
        'Grade': 75.23423,
        'SAT': 873.42342,
        ...
    },
    ...
]</pre>
</div>
</li>
<li><p class="first"><em>pvalparam</em> &#8211; The p-value below which to consider something significantly unlikely. A common number used is 0.05</p>
</li>
<li><p class="first"><em>indegree</em> &#8211; The upper bound on the size of a witness set (see Koller et al. 85). If this is larger than 1, a huge amount of trials are required to avoid a divide-by- zero error.</p>
</li>
</ol>
</dd>
</dl>
<p>Usage example: this would learn entire Bayesian networks from sets of 8000 data points:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">libpgm.nodedata</span> <span class="kn">import</span> <span class="n">NodeData</span>
<span class="kn">from</span> <span class="nn">libpgm.graphskeleton</span> <span class="kn">import</span> <span class="n">GraphSkeleton</span>
<span class="kn">from</span> <span class="nn">libpgm.lgbayesiannetwork</span> <span class="kn">import</span> <span class="n">LGBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.discretebayesiannetwork</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="kn">from</span> <span class="nn">libpgm.pgmlearner</span> <span class="kn">import</span> <span class="n">PGMLearner</span>

<span class="c"># LINEAR GAUSSIAN</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestlgdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">lgbn</span> <span class="o">=</span> <span class="n">LGBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">lgbn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">8000</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># learn bayesian network</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">lg_estimatebn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">Vdata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c"># DISCRETE</span>

<span class="c"># generate some data to use</span>
<span class="n">nd</span> <span class="o">=</span> <span class="n">NodeData</span><span class="p">()</span>
<span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>    <span class="c"># an input file</span>
<span class="n">skel</span> <span class="o">=</span> <span class="n">GraphSkeleton</span><span class="p">()</span>
<span class="n">skel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;../tests/unittestdict.txt&quot;</span><span class="p">)</span>
<span class="n">skel</span><span class="o">.</span><span class="n">toporder</span><span class="p">()</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">(</span><span class="n">skel</span><span class="p">,</span> <span class="n">nd</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">randomsample</span><span class="p">(</span><span class="mi">8000</span><span class="p">)</span>

<span class="c"># instantiate my learner </span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">PGMLearner</span><span class="p">()</span>

<span class="c"># learn bayesian network</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">discrete_estimatebn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># output</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">E</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">Vdata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="sampleaggregator.html"
                        title="previous chapter">sampleaggregator</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="CPDtypes.html"
                        title="next chapter">CPDtypes</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/pgmlearner.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="CPDtypes.html" title="CPDtypes"
             >next</a> |</li>
        <li class="right" >
          <a href="sampleaggregator.html" title="sampleaggregator"
             >previous</a> |</li>
        <li><a href="index.html">libpgm 1.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, CyberPoint International, LLC.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>