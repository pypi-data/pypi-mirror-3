
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Using the Cluster &mdash; StarCluster v0.92 documentation</title>
    <link rel="stylesheet" href="../_static/pylons.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.92',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="StarCluster v0.92 documentation" href="../index.html" />
    <link rel="up" title="StarCluster User Manual" href="index.html" />
    <link rel="next" title="Customizing the StarCluster AMI" href="create_new_ami.html" />
    <link rel="prev" title="Copying Data to and from a Cluster" href="putget.html" />
<link rel="stylesheet" href="../_static/nobile.css" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="../_static/neuton.css" type="text/css" media="screen" charset="utf-8" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="../_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->
<link rel="shortcut icon" href="../_static/starcluster.ico"/>

  </head>
  <body>
<div class="header-small">
	
	<div class="logo-small">
		<a href="../index.html">
      		<img class="logo" src="../_static/logo-small.png" alt="Logo"/>
		</a>
  	</div>
</div>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="create_new_ami.html" title="Customizing the StarCluster AMI"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="putget.html" title="Copying Data to and from a Cluster"
             accesskey="P">previous</a> |</li>
    	<li><a href="../index.html">Home</a> &raquo;</li>
          <li><a href="../contents.html" >Master Table of Contents</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">StarCluster User Manual</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="using-the-cluster">
<h1>Using the Cluster<a class="headerlink" href="#using-the-cluster" title="Permalink to this headline">¶</a></h1>
<p>After you&#8217;ve created a StarCluster on Amazon, it&#8217;s time to login and do some
real work.  The sections below explain how to access the cluster, verify that
everything&#8217;s configured properly, and how to use OpenMPI and Sun Grid Engine on
StarCluster.</p>
<p>For these sections we used a small two node StarCluster for demonstration. In
some cases, you may need to adjust the instructions/commands for your size
cluster. For all sections, we assume <strong>cluster_user</strong> is set to <em>sgeadmin</em>.  If
you&#8217;ve specified a different <strong>cluster_user</strong>, please replace sgeadmin with
your <strong>cluster_user</strong> in the following sections.</p>
<div class="section" id="logging-into-the-master-node">
<h2>Logging into the master node<a class="headerlink" href="#logging-into-the-master-node" title="Permalink to this headline">¶</a></h2>
<p>To login to the master node as root:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

The authenticity of host 'ec2-123-123-123-231.compute-1.amazonaws.com (123.123.123.231)' can't be established.
RSA key fingerprint is 85:23:b0:7e:23:c8:d1:02:4f:ba:22:53:42:d5:e5:23.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-123-123-123-231.compute-1.amazonaws.com,123.123.123.231' (RSA) to the list of known hosts.
Last login: Wed May 12 00:13:51 2010 from 192.168.1.1

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/\*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To access official Ubuntu documentation, please visit:
http://help.ubuntu.com/

Created From:
Amazon EC2 Ubuntu 9.10 jaunty AMI built by Eric Hammond
http://alestic.com http://ec2ubuntu-group.notlong.com

StarCluster EC2 AMI created by Justin Riley (MIT)
url: http://web.mit.edu/stardev/cluster
email: star 'at' mit 'dot' edu
root@master:~#</pre>
</div>
<p>This command is used frequently in the sections below to ensure that you&#8217;re
logged into the master node of a StarCluster on Amazon&#8217;s EC2 as root.</p>
</div>
<div class="section" id="logging-into-a-worker-node">
<h2>Logging into a worker node<a class="headerlink" href="#logging-into-a-worker-node" title="Permalink to this headline">¶</a></h2>
<p>You also have the option of logging into any particular worker node as root by
using the <strong>sshnode</strong> command. First, run &#8220;starcluster listclusters&#8221; to list
the nodes:</p>
<div class="highlight-python"><pre>$ starcluster listclusters
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

---------------------------------------------------
mycluster (security group: @sc-mycluster)
---------------------------------------------------
Launch time: 2010-02-19T20:55:20.000Z
Zone: us-east-1c
Keypair: gsg-keypair
EBS volumes:
    vol-c8888888 on master:/dev/sdj (status: attached)
Cluster nodes:
     master i-99999999 running ec2-123-123-123-121.compute-1.amazonaws.com
    node001 i-88888888 running ec2-123-123-123-122.compute-1.amazonaws.com
    node002 i-88888888 running ec2-123-23-23-24.compute-1.amazonaws.com
    node003 i-77777777 running ec2-123-23-23-25.compute-1.amazonaws.com
    ....</pre>
</div>
<p>Then use &#8220;starcluster sshnode mycluster&#8221; to login to a node:</p>
<div class="highlight-python"><pre>$ starcluster sshnode mycluster node001
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

The authenticity of host 'ec2-123-123-123-232.compute-1.amazonaws.com (123.123.123.232)' can't be established.
RSA key fingerprint is 86:23:b0:7e:23:c8:d1:02:4f:ba:22:53:42:d5:e5:23.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ec2-123-123-123-232.compute-1.amazonaws.com,123.123.123.232' (RSA) to the list of known hosts.
Last login: Wed May 12 00:13:51 2010 from 192.168.1.1

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/\*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To access official Ubuntu documentation, please visit:
http://help.ubuntu.com/

Created From:
Amazon EC2 Ubuntu 9.04 jaunty AMI built by Eric Hammond
http://alestic.com http://ec2ubuntu-group.notlong.com

StarCluster EC2 AMI created by Justin Riley (MIT)
url: http://web.mit.edu/stardev/cluster
email: star 'at' mit 'dot' edu

0 packages can be updated.
0 updates are security updates.

root@node001:~#</pre>
</div>
</div>
<div class="section" id="verify-etc-hosts">
<h2>Verify /etc/hosts<a class="headerlink" href="#verify-etc-hosts" title="Permalink to this headline">¶</a></h2>
<p>Once StarCluster is up, the /etc/hosts file should look like:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:~# cat /etc/hosts
# Do not remove the following line or programs that require network functionality will fail
127.0.0.1 localhost.localdomain localhost
10.252.167.143 master
10.252.165.173 node001</pre>
</div>
<p>As you can see, the head node is assigned an alias of &#8216;master&#8217; and each node
after that is labeled node001, node002, etc.</p>
<p>In this example we have two nodes so only master and node001 are in /etc/hosts.</p>
</div>
<div class="section" id="verify-passwordless-ssh">
<h2>Verify Passwordless SSH<a class="headerlink" href="#verify-passwordless-ssh" title="Permalink to this headline">¶</a></h2>
<p>StarCluster should have automatically setup passwordless ssh for both root and
the CLUSTER_USER you specified.</p>
<p>To test this out, let&#8217;s login to the master node and attempt to run the
hostname command via SSH on node001 without a password for both root and
sgeadmin (ie CLUSTER_USER):</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:~# ssh node001 hostname
node001
root@master:~# su - sgeadmin
sgeadmin@master:~# ssh node001 hostname
node001
sgeadmin@master:~# exit
root@master:~#</pre>
</div>
</div>
<div class="section" id="verify-home-is-nfs-shared">
<h2>Verify /home is NFS Shared<a class="headerlink" href="#verify-home-is-nfs-shared" title="Permalink to this headline">¶</a></h2>
<p>The /home folder on all clusters launched by StarCluster should be NFS shared
to each node. To check this, login to the master as root and run the mount
command on each node to verify that /home is mounted from the master:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:~# ssh node001 mount
/dev/sda1 on / type ext3 (rw)
none on /proc type proc (rw)
none on /sys type sysfs (rw)
/dev/sda2 on /mnt type ext3 (rw)
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
master:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)</pre>
</div>
<p>The last line in the output above indicates that /home is mounted from the
master node over NFS. Running this for the rest of the nodes (e.g. node002,
node003, etc) should produce the same output.</p>
</div>
<div class="section" id="ensure-ebs-volumes-are-mounted-and-nfs-shared-optional">
<h2>Ensure EBS Volumes are Mounted and NFS shared (OPTIONAL)<a class="headerlink" href="#ensure-ebs-volumes-are-mounted-and-nfs-shared-optional" title="Permalink to this headline">¶</a></h2>
<p>If you chose to use EBS for persistent storage (recommended) you should check
that it is mounted and shared across the cluster via NFS at the location you
specified in the config.  To do this we login to the master and run a few
commands to ensure everything is working properly.  For this example we assume
that a single 20GB volume has been attached to the cluster and that the volume
has <em>MOUNT_PATH=/home</em> in the config. If you&#8217;ve attached multiple EBS volumes
to the cluster, you should repeat these checks for each volume you specified in
the config.</p>
<p>The first thing we want to do is to make sure the device was actually attached
to the master node as a device. To check that the device is attached on the
master node, we login to the master and use &#8220;fdisk -l&#8221; to look for our volume:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster

root@master:~# fdisk -l

...

Disk /dev/sdz: 21.4 GB, 21474836480 bytes
255 heads, 63 sectors/track, 2610 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Disk identifier: 0x2a2a3cscg

Device Boot Start End Blocks Id System
/dev/sdz1 1 2610 20964793+ 83 Linux</pre>
</div>
<p>From the output of fdisk above we see that there is indeed a 20GB device
/dev/sdz with partition /dev/sdz1 attached on the master node.</p>
<p>Next check the output of mount on the master node to ensure that the volume&#8217;s
<em>PARTITION</em> setting (which defaults to 1 if not specified) has been mounted to
the volume&#8217;s <em>MOUNT_PATH</em> setting specified in the config (/home for this
example):</p>
<div class="highlight-python"><pre>root@master:~# mount
...
/dev/sdz1 on /home type ext3 (rw)
...</pre>
</div>
<p>From the output of mount we see that the partition /dev/sdz1 has been mounted
to /home on the master node as we specified in the config.</p>
<p>Finally we check that the <em>MOUNT_PATH</em> specified in the config for this volume
has been NFS shared to each cluster node by running mount on each node and
examining the output:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:~# ssh node001 mount
/dev/sda1 on / type ext3 (rw)
none on /proc type proc (rw)
none on /sys type sysfs (rw)
/dev/sda2 on /mnt type ext3 (rw)
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
master:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)
root@master:~# ssh node002 mount
...
master:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)
...</pre>
</div>
<p>The last line in the output above indicates that <em>MOUNT_PATH</em> (/home for this
example) is mounted on each worker node from the master node via NFS.  Running
this for the rest of the nodes (e.g. node002, node003, etc) should produce the
same output.</p>
</div>
<div class="section" id="verify-scratch-space">
<h2>Verify scratch space<a class="headerlink" href="#verify-scratch-space" title="Permalink to this headline">¶</a></h2>
<p>Each node should be set up with approximately 140GB or more of local scratch
space for writing temporary files instead of storing temporary files on NFS.
The location of the scratch space is /scratch/CLUSTER_USER. So, for this
example the local scratch for CLUSTER_USER=sgeadmin is /scratch/sgeadmin.</p>
<p>To verify this, login to the master and run &#8220;ls -l /scratch&#8221;:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:/# ls -l /scratch/
total 0
lrwxrwxrwx 1 root root 13 2009-09-09 14:34 sgeadmin -&gt; /mnt/sgeadmin</pre>
</div>
<p>From the output above we see that /scratch/sgeadmin has been symbolically
linked to /mnt/sgeadmin</p>
<p>Next we run the df command to verify that at least ~140GB is available on /mnt
(and thus /mnt/sgeadmin):</p>
<div class="highlight-python"><pre>root@master:~# df -h
Filesystem Size Used Avail Use% Mounted on
...
/dev/sda2 147G 188M 140G 1% /mnt
...
root@master:~#</pre>
</div>
</div>
<div class="section" id="compile-and-run-a-hello-world-openmpi-program">
<h2>Compile and run a &#8220;Hello World&#8221; OpenMPI program<a class="headerlink" href="#compile-and-run-a-hello-world-openmpi-program" title="Permalink to this headline">¶</a></h2>
<p>Below is a simple Hello World program in MPI</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cp">#include &lt;stdio.h&gt; </span><span class="cm">/* printf and BUFSIZ defined there */</span><span class="cp"></span>
<span class="cp">#include &lt;stdlib.h&gt; </span><span class="cm">/* exit defined there */</span><span class="cp"></span>
<span class="cp">#include &lt;mpi.h&gt; </span><span class="cm">/* all MPI-2 functions defined there */</span><span class="cp"></span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">)</span>
        <span class="kt">int</span> <span class="n">argc</span><span class="p">;</span>
        <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[];</span>
        <span class="p">{</span>
        <span class="kt">int</span> <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">length</span><span class="p">;</span>
        <span class="kt">char</span> <span class="n">name</span><span class="p">[</span><span class="n">BUFSIZ</span><span class="p">];</span>

        <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
        <span class="n">MPI_Get_processor_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%s: hello world from process %d of %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

        <span class="n">MPI_Finalize</span><span class="p">();</span>

        <span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Save this code to a file called helloworldmpi.c in /home/sgeadmin. You can then
compile and run the code across the cluster like so:</p>
<div class="highlight-python"><pre>$ starcluster sshmaster mycluster
root@master:~# su - sgeadmin
sgeadmin@master:~$ mpicc helloworldmpi.c -o helloworldmpi
sgeadmin@master:~$ mpirun -n 2 -host master,node001 ./helloworldmpi
master: hello world from process 0 of 2
node001: hello world from process 1 of 2
sgeadmin@master:~$</pre>
</div>
<p>Obviously if you have more nodes, the -host mater,node001 list specified will
need to be extended. You can also create a hostfile instead of listing each
node for OpenMPI to use that looks like:</p>
<div class="highlight-python"><pre>sgeadmin@:~$ cat /home/sgeadmin/hostfile
master
node001</pre>
</div>
<p>After creating this hostfile, you can now call mpirun with less options:</p>
<div class="highlight-python"><pre>sgeadmin@master:~$ mpirun -n 2 -hostfile /home/sgeadmin/hostfile ./helloworldmpi
master: hello world from process 0 of 2
node001: hello world from process 1 of 2
sgeadmin@master:~$</pre>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Using the Cluster</a><ul>
<li><a class="reference internal" href="#logging-into-the-master-node">Logging into the master node</a></li>
<li><a class="reference internal" href="#logging-into-a-worker-node">Logging into a worker node</a></li>
<li><a class="reference internal" href="#verify-etc-hosts">Verify /etc/hosts</a></li>
<li><a class="reference internal" href="#verify-passwordless-ssh">Verify Passwordless SSH</a></li>
<li><a class="reference internal" href="#verify-home-is-nfs-shared">Verify /home is NFS Shared</a></li>
<li><a class="reference internal" href="#ensure-ebs-volumes-are-mounted-and-nfs-shared-optional">Ensure EBS Volumes are Mounted and NFS shared (OPTIONAL)</a></li>
<li><a class="reference internal" href="#verify-scratch-space">Verify scratch space</a></li>
<li><a class="reference internal" href="#compile-and-run-a-hello-world-openmpi-program">Compile and run a &#8220;Hello World&#8221; OpenMPI program</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="putget.html"
                        title="previous chapter">Copying Data to and from a Cluster</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="create_new_ami.html"
                        title="next chapter">Customizing the StarCluster AMI</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="create_new_ami.html" title="Customizing the StarCluster AMI"
             >next</a> |</li>
        <li class="right" >
          <a href="putget.html" title="Copying Data to and from a Cluster"
             >previous</a> |</li>
    	<li><a href="../index.html">Home</a> &raquo;</li>
          <li><a href="../contents.html" >Master Table of Contents</a> &raquo;</li>
          <li><a href="index.html" >StarCluster User Manual</a> &raquo;</li> 
      </ul>
    </div>


    <div class="footer">
        &copy; Copyright 2010, Software Tools for Academics and Researchers.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>


<script type="text/javascript">
//<![CDATA[
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    //]]>
    </script><script type="text/javascript">
//<![CDATA[
    try{
        var pageTracker = _gat._getTracker("UA-1048253-12");
        pageTracker._trackPageview();
    } catch(err) {}
    //]]>
    </script>

  </body>
</html>